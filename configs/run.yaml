# Training Run Configuration

# Experiment Settings
experiment:
  name: "ttm_vitaldb_fm"
  seed: 42
  deterministic: true
  log_level: "INFO"
  
  # Weights & Biases logging (optional)
  wandb:
    enabled: false
    project: "ttm-vitaldb"
    entity: null
    tags: ["fm", "vitaldb"]

# Data Loading
data:
  # Batch sizes
  batch_size: 32
  val_batch_size: 64
  test_batch_size: 64
  
  # DataLoader settings
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  
  # Sampling strategy
  sampling:
    strategy: "balanced"  # balanced, weighted, or uniform
    oversample_minority: false
    undersample_majority: false
  
  # Data augmentation (applied during loading)
  augmentation:
    enabled: false
    mixup:
      alpha: 0.2
      enabled: false
    cutmix:
      alpha: 1.0
      enabled: false

# Optimization
optimization:
  # Optimizer
  optimizer: "adamw"
  learning_rate: 5e-4
  weight_decay: 0.01
  
  # Adam-specific parameters
  adam:
    betas: [0.9, 0.999]
    eps: 1e-8
    amsgrad: false
  
  # Learning rate schedule
  scheduler:
    type: "cosine"  # cosine, linear, exponential, reduce_on_plateau
    
    # Cosine annealing
    cosine:
      T_max: 50  # Maximum epochs
      eta_min: 1e-6  # Minimum LR
      
    # Warmup
    warmup:
      enabled: true
      epochs: 2
      start_factor: 0.1
    
    # Reduce on plateau
    reduce_on_plateau:
      factor: 0.5
      patience: 3
      min_lr: 1e-7
      monitor: "val_loss"
  
  # Gradient accumulation
  gradient_accumulation_steps: 1
  
  # Gradient clipping
  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"  # norm or value

# Training Settings
training:
  epochs: 50
  
  # Validation
  val_check_interval: 1.0  # Check every epoch
  val_portion: 0.1  # Use 10% as validation if no val split
  
  # Checkpointing
  checkpoint:
    save_top_k: 3
    monitor: "val_accuracy"  # Metric to monitor
    mode: "max"  # max or min
    save_last: true
    save_weights_only: false
    
  # Early stopping
  early_stopping:
    enabled: true
    patience: 5
    min_delta: 0.001
    monitor: "val_loss"
    mode: "min"
  
  # Logging
  log_every_n_steps: 10
  progress_bar: true
  
  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "artifacts/tensorboard"
    log_graph: false  # Log model graph
    log_images: false  # Log sample predictions

# Evaluation
evaluation:
  # Metrics to compute
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "auroc"
    - "auprc"
    - "confusion_matrix"
    - "per_class_accuracy"
  
  # Calibration
  calibration:
    enabled: true
    method: "temperature"  # temperature, isotonic, platt
    n_bins: 10  # For ECE computation
    
  # Test-time behavior
  test:
    overlap: 0.0  # Window overlap ratio
    voting: "soft"  # For overlapping predictions
    tta_enabled: false  # Test-time augmentation
    
  # Threshold optimization
  threshold:
    optimize: true
    metric: "f1"  # Metric to optimize
    search_space: [0.3, 0.7]  # Search range
    n_thresholds: 100

# Cross-Validation (Optional)
cv:
  enabled: false
  n_folds: 5
  stratified: true
  
  # Fold-specific settings
  fold_seed_offset: 1000  # Add to base seed for each fold
  aggregate_metrics: "mean"  # mean, median, or max
  save_all_folds: false

# Distributed Training
distributed:
  enabled: false
  backend: "nccl"  # nccl, gloo, mpi
  
  # DDP settings
  find_unused_parameters: false
  gradient_as_bucket_view: true
  static_graph: false
  
  # Mixed precision with DDP
  ddp_mixed_precision:
    enabled: true
    backend: "native"  # native or apex

# Hardware
hardware:
  # GPU settings
  gpus: 1  # Number of GPUs (-1 for all)
  
  # Memory optimization
  empty_cache_interval: 100  # Steps
  
  # CPU settings (if no GPU)
  cpu:
    num_threads: 8
    
# FastTrack Mode Overrides
fasttrack:
  # These settings override above when --fasttrack is used
  data:
    batch_size: 64
    num_workers: 2
  
  optimization:
    learning_rate: 1e-3  # Higher LR for frozen encoder
    scheduler:
      warmup:
        enabled: false
  
  training:
    epochs: 10
    early_stopping:
      patience: 3
    log_every_n_steps: 5
  
  evaluation:
    calibration:
      enabled: false  # Skip for speed
    threshold:
      optimize: false

# Debugging
debug:
  enabled: false
  
  # Debug options
  fast_dev_run: false  # Run 1 batch
  overfit_batches: 0  # Overfit N batches
  track_grad_norm: -1  # -1 to disable
  
  # Profiling
  profiler: null  # simple, advanced, or pytorch
  
  # Deterministic mode (slower but reproducible)
  deterministic:
    enabled: true
    warn_only: true

# Paths
paths:
  # Data paths
  data_dir: "data"
  cache_dir: "cache"
  
  # Output paths  
  output_dir: "artifacts"
  checkpoint_dir: "artifacts/checkpoints"
  log_dir: "artifacts/logs"
  
  # Config paths
  config_dir: "configs"
  
# Environment Variables
env:
  # PyTorch
  TORCH_HOME: "~/.cache/torch"
  
  # CUDA
  CUDA_LAUNCH_BLOCKING: 0
  
  # Debugging
  PYTHONWARNINGS: "ignore"

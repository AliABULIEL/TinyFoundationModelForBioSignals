# High-Accuracy Model Configuration

model:
  variant: "ibm-granite/granite-timeseries-ttm-r1"
  input_channels: 3
  context_length: 1250
  d_model: 512
  patch_size: 16
  stride: 8

task:
  type: "classification"
  num_classes: 2

# Fine-tuning with partial unfreezing
training_mode:
  freeze_encoder: false
  unfreeze_last_n_blocks: 2  # Unfreeze last 2 transformer blocks
  
  # Gradual unfreezing
  gradual_unfreeze:
    enabled: true
    schedule: [0, 0, 2, 4, 6]  # Progressive unfreezing

# LoRA for efficient adaptation
lora:
  enabled: true
  r: 16  # Higher rank for more capacity
  alpha: 32
  dropout: 0.1
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"

# Enhanced MLP head
head:
  type: "mlp"
  mlp:
    hidden_dims: [512, 256, 128]
    activation: "gelu"
    dropout: 0.2
    use_batch_norm: false  # Avoid dimension issues

# Advanced loss function
loss:
  classification:
    type: "focal"  # Better for imbalanced data
    focal:
      alpha: 0.25
      gamma: 2.0
    label_smoothing: 0.1

regularization:
  weight_decay: 0.01
  gradient_clip: 1.0
  early_stopping:
    patience: 10
    monitor: "val_f1"
    mode: "max"

# Mixed precision
mixed_precision:
  enabled: true
  opt_level: "O1"

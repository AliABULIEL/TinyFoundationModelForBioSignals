# VAE Model Configuration for Biosignal Representation Learning

# Model selection
model_type: vae  # 'ttm' for pre-trained TTM, 'vae' for trainable VAE

# VAE Architecture
latent_dim: 64  # Size of latent representation
beta: 1.0       # Beta parameter for beta-VAE (controls KL weight)

# Task configuration
task: classification  # 'classification', 'regression', or 'reconstruction'
num_classes: 2       # For classification tasks
out_features: 1      # For regression tasks

# Training mode
freeze_encoder: false  # Set to true after unsupervised pretraining
head_type: mlp        # 'linear' or 'mlp' for downstream task head
dropout_rate: 0.2     # Dropout rate for MLP head

# Input configuration (automatically set by pipeline)
# input_channels: 1    # Set by data pipeline
# context_length: 1250 # Set by data pipeline (10s @ 125Hz)

# Advanced VAE settings
vae_loss_weight: 0.5  # Weight VAE loss vs task loss during fine-tuning
reconstruction_loss: mse  # 'mse' or 'l1'
kl_annealing: false      # Gradually increase KL weight during training
min_kl_weight: 0.01      # Minimum KL weight if annealing
max_kl_weight: 1.0       # Maximum KL weight if annealing
annealing_epochs: 10     # Epochs to anneal KL weight

# Model initialization
init_weights: xavier  # 'xavier', 'kaiming', or 'normal'
init_gain: 1.0       # Gain for initialization

# Architecture details
encoder_hidden_dims: [32, 64, 128, 256]  # Encoder conv channels
decoder_hidden_dims: [256, 128, 64, 32]  # Decoder conv channels
kernel_size: 7          # Convolutional kernel size
stride: 2              # Stride for conv layers
padding: 3             # Padding for conv layers
use_batch_norm: true   # Use batch normalization
use_dropout: true      # Use dropout in encoder/decoder
dropout_conv: 0.1      # Dropout rate in conv layers

# Comparison with TTM
# TTM advantages: Pre-trained on massive dataset, better zero-shot performance
# VAE advantages: Trainable from scratch, interpretable latent space, generates samples
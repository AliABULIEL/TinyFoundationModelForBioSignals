{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTM-HAR: Foundation Model Research Notebook\n",
    "\n",
    "## Tiny Time Mixer for Human Activity Recognition on CAPTURE-24\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "```\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  PART 1: VALIDATION (Fast, ~1 minute)                                      ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïë\n",
    "‚ïë  ‚Ä¢ Environment verification                                                ‚ïë\n",
    "‚ïë  ‚Ä¢ TTM installation check                                                  ‚ïë\n",
    "‚ïë  ‚Ä¢ Pipeline smoke test                                                     ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  PART 2: RESEARCH (Slow, ~1-4 hours)                                       ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïë\n",
    "‚ïë  ‚Ä¢ Real CAPTURE-24 data loading                                            ‚ïë\n",
    "‚ïë  ‚Ä¢ Linear probe training                                                   ‚ïë\n",
    "‚ïë  ‚Ä¢ Full fine-tuning                                                        ‚ïë\n",
    "‚ïë  ‚Ä¢ Comprehensive evaluation                                                ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "```\n",
    "\n",
    "### Repository Modules Used\n",
    "\n",
    "| Import | Module | Purpose |\n",
    "|--------|--------|----------|\n",
    "| `validate_ttm_available` | `src.models.ttm_wrapper` | TTM installation check |\n",
    "| `TTMWrapper` | `src.models.ttm_wrapper` | TTM backbone wrapper |\n",
    "| `create_model`, `HARModel` | `src.models.model_factory` | Model creation |\n",
    "| `get_classification_head` | `src.models.heads` | Classification head |\n",
    "| `get_device` | `src.utils.device` | Hardware detection |\n",
    "| `set_seed` | `src.utils.reproducibility` | Reproducibility |\n",
    "| `compute_metrics`, `classification_report` | `src.evaluation.metrics` | Metrics |\n",
    "\n",
    "### Critical Constraint\n",
    "\n",
    "```\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  This notebook uses REAL IBM TTM ONLY ‚Äî NO MOCKS, NO FALLBACKS              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PART 1: VALIDATION\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "**Runtime**: ~1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: System Path Setup\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path.cwd()\n",
    "if REPO_ROOT.name == \"notebooks\":\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: TTM Verification (src.models.ttm_wrapper)\n",
    "# =============================================================================\n",
    "\n",
    "from src.models.ttm_wrapper import is_ttm_available, validate_ttm_available\n",
    "\n",
    "# This raises ImportError with detailed instructions if TTM not installed\n",
    "validate_ttm_available()\n",
    "\n",
    "# Get TTM class for direct backbone loading\n",
    "try:\n",
    "    from tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\n",
    "    TTM_SOURCE = \"tsfm_public\"\n",
    "except ImportError:\n",
    "    from granite_tsfm.models import TinyTimeMixerForPrediction\n",
    "    TTM_SOURCE = \"granite_tsfm\"\n",
    "\n",
    "TTM_CLASS = TinyTimeMixerForPrediction\n",
    "print(f\"‚úÖ TTM verified: {TTM_SOURCE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Core Dependencies\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "print(f\"numpy:  {np.__version__}\")\n",
    "print(f\"torch:  {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Hardware Detection (src.utils.device)\n",
    "# =============================================================================\n",
    "\n",
    "from src.utils.device import get_device\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Reproducibility (src.utils.reproducibility)\n",
    "# =============================================================================\n",
    "\n",
    "from src.utils.reproducibility import set_seed\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"‚úÖ Seed set: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Configuration\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    \"model\": {\n",
    "        \"backbone\": \"ttm\",\n",
    "        \"checkpoint\": \"ibm-granite/granite-timeseries-ttm-r2\",\n",
    "        \"num_channels\": 3,\n",
    "        \"num_classes\": 5,\n",
    "        \"context_length\": 512,\n",
    "        \"patch_length\": 16,\n",
    "        \"freeze_strategy\": \"all\",\n",
    "        \"head\": {\"type\": \"linear\", \"dropout\": 0.1},\n",
    "    },\n",
    "    \"dataset\": {\"data_path\": \"data/capture24\"},\n",
    "    \"training\": {\"batch_size\": 64, \"num_workers\": 4},\n",
    "}\n",
    "\n",
    "# Load from YAML if exists\n",
    "CONFIG_PATH = REPO_ROOT / \"configs\" / \"default.yaml\"\n",
    "if CONFIG_PATH.exists():\n",
    "    with open(CONFIG_PATH) as f:\n",
    "        yaml_config = yaml.safe_load(f)\n",
    "        for key in yaml_config:\n",
    "            if key in CONFIG:\n",
    "                CONFIG[key].update(yaml_config[key])\n",
    "            else:\n",
    "                CONFIG[key] = yaml_config[key]\n",
    "    print(f\"‚úÖ Loaded config from {CONFIG_PATH}\")\n",
    "\n",
    "CLASS_NAMES = [\"Sleep\", \"Sedentary\", \"Light\", \"Moderate\", \"Vigorous\"]\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "print(f\"Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Model Creation (src.models.model_factory)\n",
    "# =============================================================================\n",
    "\n",
    "from src.models.model_factory import create_model, HARModel\n",
    "from src.models.ttm_wrapper import TTMWrapper\n",
    "from src.models.heads import get_classification_head\n",
    "\n",
    "# Create model using the factory (validates no mocks)\n",
    "model = create_model(CONFIG)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úÖ Model created via src.models.model_factory.create_model()\")\n",
    "print(f\"   Total params:     {total_params:,}\")\n",
    "print(f\"   Trainable params: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Validation Smoke Test\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION SMOKE TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_size = 8\n",
    "context_length = CONFIG[\"model\"][\"context_length\"]\n",
    "num_channels = CONFIG[\"model\"][\"num_channels\"]\n",
    "\n",
    "test_inputs = torch.randn(batch_size, context_length, num_channels).to(DEVICE)\n",
    "test_labels = torch.randint(0, NUM_CLASSES, (batch_size,)).to(DEVICE)\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_inputs)\n",
    "\n",
    "print(f\"\\n‚úÖ Forward: {test_inputs.shape} ‚Üí {outputs.shape}\")\n",
    "assert outputs.shape == (batch_size, NUM_CLASSES)\n",
    "assert not torch.isnan(outputs).any()\n",
    "\n",
    "# Backward pass\n",
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "outputs = model(test_inputs)\n",
    "loss = criterion(outputs, test_labels)\n",
    "loss.backward()\n",
    "\n",
    "print(f\"‚úÖ Backward: loss = {loss.item():.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ VALIDATION PASSED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# ‚ö†Ô∏è RESEARCH MODE - Long Runtime (1-4 hours)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Data Loading (src.data.datamodule)\n",
    "# =============================================================================\n",
    "#\n",
    "# Research mode REQUIRES real CAPTURE-24 data.\n",
    "# Synthetic data is for unit testing only (see src/data/synthetic.py)\n",
    "#\n",
    "\n",
    "from src.data.datamodule import HARDataModule\n",
    "\n",
    "DATA_PATH = REPO_ROOT / CONFIG[\"dataset\"].get(\"data_path\", \"data/capture24\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "\n",
    "# Check for real data\n",
    "participant_files = list(DATA_PATH.glob(\"P*.csv.gz\")) + list(DATA_PATH.glob(\"P*.csv\"))\n",
    "\n",
    "if len(participant_files) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        f\"\\n\" + \"=\" * 70 + \"\\n\"\n",
    "        f\"‚ùå CAPTURE-24 DATA NOT FOUND\\n\"\n",
    "        f\"=\" * 70 + \"\\n\\n\"\n",
    "        f\"Research mode requires real CAPTURE-24 data.\\n\\n\"\n",
    "        f\"Expected location: {DATA_PATH}\\n\"\n",
    "        f\"Expected files: P*.csv.gz or P*.csv\\n\\n\"\n",
    "        f\"To download CAPTURE-24:\\n\"\n",
    "        f\"  1. Visit: https://ora.ox.ac.uk/objects/uuid:99d7c092-d865-4a19-b096-cc16f6bab45f\\n\"\n",
    "        f\"  2. Download and extract to: {DATA_PATH}\\n\\n\"\n",
    "        f\"For quick validation without data, run Part 1 only.\\n\"\n",
    "        f\"=\" * 70\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Found {len(participant_files)} participant files\")\n",
    "print(\"‚úÖ Using src.data.datamodule.HARDataModule\")\n",
    "\n",
    "# Create datamodule\n",
    "datamodule = HARDataModule(CONFIG)\n",
    "datamodule.setup()\n",
    "\n",
    "train_dataset = datamodule.train_dataset\n",
    "val_dataset = datamodule.val_dataset\n",
    "test_dataset = datamodule.test_dataset\n",
    "class_weights = torch.tensor(datamodule.class_weights, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "print(f\"\\nDatasets: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: DataLoaders\n",
    "# =============================================================================\n",
    "\n",
    "BATCH_SIZE = CONFIG[\"training\"].get(\"batch_size\", 64)\n",
    "NUM_WORKERS = 0 if DEVICE.type != \"cuda\" else CONFIG[\"training\"].get(\"num_workers\", 4)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=(DEVICE.type == \"cuda\"), drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=(DEVICE.type == \"cuda\")\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=(DEVICE.type == \"cuda\")\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders: Train={len(train_loader)}, Val={len(val_loader)}, Test={len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: Training Functions (src.evaluation.metrics)\n",
    "# =============================================================================\n",
    "#\n",
    "# For full production training: src.training.trainer.Trainer\n",
    "#\n",
    "\n",
    "from src.evaluation.metrics import compute_metrics, classification_report as src_classification_report\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
    "    \"\"\"Train one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    return total_loss / len(loader), metrics[\"accuracy\"], metrics[\"balanced_accuracy\"]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate using src.evaluation.metrics.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds, all_labels = np.array(all_preds), np.array(all_labels)\n",
    "    metrics = compute_metrics(all_labels, all_preds)\n",
    "    \n",
    "    return (total_loss / len(loader), metrics[\"accuracy\"], \n",
    "            metrics[\"balanced_accuracy\"], metrics[\"macro_f1\"], all_preds, all_labels)\n",
    "\n",
    "print(\"‚úÖ Training functions using src.evaluation.metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Experiment A: Linear Probe (Frozen Backbone)\n",
    "\n",
    "**Strategy**: Freeze TTM backbone, train only classification head\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: Experiment A - Linear Probe\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT A: LINEAR PROBE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create fresh model with frozen backbone using factory\n",
    "set_seed(SEED)\n",
    "config_lp = CONFIG.copy()\n",
    "config_lp[\"model\"] = CONFIG[\"model\"].copy()\n",
    "config_lp[\"model\"][\"freeze_strategy\"] = \"all\"  # Freeze backbone\n",
    "\n",
    "model_lp = create_model(config_lp).to(DEVICE)\n",
    "\n",
    "LP_EPOCHS = 20\n",
    "LP_LR = 1e-3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model_lp.parameters()),\n",
    "    lr=LP_LR, weight_decay=0.01\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=LP_EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler() if DEVICE.type == \"cuda\" else None\n",
    "\n",
    "lp_history = {\"train_loss\": [], \"val_bal_acc\": [], \"val_f1\": []}\n",
    "best_val_bal_acc = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(f\"\\nTraining {LP_EPOCHS} epochs...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(LP_EPOCHS):\n",
    "    train_loss, _, _ = train_epoch(model_lp, train_loader, criterion, optimizer, DEVICE, scaler)\n",
    "    val_loss, val_acc, val_bal_acc, val_f1, _, _ = evaluate(model_lp, val_loader, criterion, DEVICE)\n",
    "    scheduler.step()\n",
    "    \n",
    "    lp_history[\"train_loss\"].append(train_loss)\n",
    "    lp_history[\"val_bal_acc\"].append(val_bal_acc)\n",
    "    lp_history[\"val_f1\"].append(val_f1)\n",
    "    \n",
    "    if val_bal_acc > best_val_bal_acc:\n",
    "        best_val_bal_acc = val_bal_acc\n",
    "        best_model_state = {k: v.cpu().clone() for k, v in model_lp.state_dict().items()}\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{LP_EPOCHS} | Loss: {train_loss:.4f} | Val Bal-Acc: {val_bal_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete in {(time.time()-start_time)/60:.1f} min\")\n",
    "print(f\"   Best Val Balanced Accuracy: {best_val_bal_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: Linear Probe - Test Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model_lp.load_state_dict(best_model_state)\n",
    "_, test_acc, test_bal_acc, test_f1, lp_preds, lp_labels = evaluate(\n",
    "    model_lp, test_loader, criterion, DEVICE\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Linear Probe Test Results:\")\n",
    "print(f\"   Accuracy:          {test_acc:.4f}\")\n",
    "print(f\"   Balanced Accuracy: {test_bal_acc:.4f}\")\n",
    "print(f\"   Macro F1:          {test_f1:.4f}\")\n",
    "print(f\"\\n{classification_report(lp_labels, lp_preds, target_names=CLASS_NAMES, digits=4)}\")\n",
    "\n",
    "lp_results = {\"accuracy\": test_acc, \"balanced_accuracy\": test_bal_acc, \n",
    "              \"macro_f1\": test_f1, \"predictions\": lp_preds, \"labels\": lp_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Experiment B: Full Fine-Tuning\n",
    "\n",
    "**Strategy**: Unfreeze backbone with differential learning rates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: Experiment B - Fine-Tuning\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT B: FULL FINE-TUNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create model with unfrozen backbone\n",
    "set_seed(SEED)\n",
    "config_ft = CONFIG.copy()\n",
    "config_ft[\"model\"] = CONFIG[\"model\"].copy()\n",
    "config_ft[\"model\"][\"freeze_strategy\"] = \"none\"  # Unfreeze backbone\n",
    "\n",
    "model_ft = create_model(config_ft).to(DEVICE)\n",
    "\n",
    "FT_EPOCHS = 30\n",
    "FT_LR_HEAD = 1e-3\n",
    "FT_LR_BACKBONE = 1e-5\n",
    "PATIENCE = 5\n",
    "\n",
    "# Differential learning rates using model.get_parameter_groups()\n",
    "param_groups = model_ft.get_parameter_groups()\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": param_groups[\"backbone\"], \"lr\": FT_LR_BACKBONE},\n",
    "    {\"params\": param_groups[\"head\"], \"lr\": FT_LR_HEAD},\n",
    "], weight_decay=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=FT_EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler() if DEVICE.type == \"cuda\" else None\n",
    "\n",
    "ft_history = {\"train_loss\": [], \"val_bal_acc\": [], \"val_f1\": []}\n",
    "best_val_bal_acc = 0\n",
    "best_model_state = None\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(f\"\\nTraining up to {FT_EPOCHS} epochs (patience={PATIENCE})...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(FT_EPOCHS):\n",
    "    train_loss, _, _ = train_epoch(model_ft, train_loader, criterion, optimizer, DEVICE, scaler)\n",
    "    val_loss, val_acc, val_bal_acc, val_f1, _, _ = evaluate(model_ft, val_loader, criterion, DEVICE)\n",
    "    scheduler.step()\n",
    "    \n",
    "    ft_history[\"train_loss\"].append(train_loss)\n",
    "    ft_history[\"val_bal_acc\"].append(val_bal_acc)\n",
    "    ft_history[\"val_f1\"].append(val_f1)\n",
    "    \n",
    "    if val_bal_acc > best_val_bal_acc:\n",
    "        best_val_bal_acc = val_bal_acc\n",
    "        best_model_state = {k: v.cpu().clone() for k, v in model_ft.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{FT_EPOCHS} | Loss: {train_loss:.4f} | Val Bal-Acc: {val_bal_acc:.4f}\")\n",
    "    \n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete in {(time.time()-start_time)/60:.1f} min\")\n",
    "print(f\"   Best Val Balanced Accuracy: {best_val_bal_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 16: Fine-Tuning - Test Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "model_ft.load_state_dict(best_model_state)\n",
    "_, test_acc, test_bal_acc, test_f1, ft_preds, ft_labels = evaluate(\n",
    "    model_ft, test_loader, criterion, DEVICE\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Fine-Tuning Test Results:\")\n",
    "print(f\"   Accuracy:          {test_acc:.4f}\")\n",
    "print(f\"   Balanced Accuracy: {test_bal_acc:.4f}\")\n",
    "print(f\"   Macro F1:          {test_f1:.4f}\")\n",
    "print(f\"\\n{classification_report(ft_labels, ft_preds, target_names=CLASS_NAMES, digits=4)}\")\n",
    "\n",
    "ft_results = {\"accuracy\": test_acc, \"balanced_accuracy\": test_bal_acc,\n",
    "              \"macro_f1\": test_f1, \"predictions\": ft_preds, \"labels\": ft_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 17: Visualization\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(lp_history[\"train_loss\"], label=\"Linear Probe\")\n",
    "axes[0, 0].plot(ft_history[\"train_loss\"], label=\"Fine-Tuning\")\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].set_title(\"Training Loss\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Balanced Accuracy\n",
    "axes[0, 1].plot(lp_history[\"val_bal_acc\"], label=\"Linear Probe\")\n",
    "axes[0, 1].plot(ft_history[\"val_bal_acc\"], label=\"Fine-Tuning\")\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"Balanced Accuracy\")\n",
    "axes[0, 1].set_title(\"Validation Balanced Accuracy\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion matrices\n",
    "for ax, preds, labels, title, results in [\n",
    "    (axes[1, 0], lp_preds, lp_labels, \"Linear Probe\", lp_results),\n",
    "    (axes[1, 1], ft_preds, ft_labels, \"Fine-Tuning\", ft_results)\n",
    "]:\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(f\"{title} (Bal-Acc: {results['balanced_accuracy']:.3f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ttm_har_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: ttm_har_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 18: Results Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"                         RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Balanced Accuracy\", \"Macro F1\"],\n",
    "    \"Linear Probe\": [f\"{lp_results['accuracy']:.4f}\", \n",
    "                      f\"{lp_results['balanced_accuracy']:.4f}\",\n",
    "                      f\"{lp_results['macro_f1']:.4f}\"],\n",
    "    \"Fine-Tuning\": [f\"{ft_results['accuracy']:.4f}\",\n",
    "                     f\"{ft_results['balanced_accuracy']:.4f}\",\n",
    "                     f\"{ft_results['macro_f1']:.4f}\"],\n",
    "})\n",
    "print(f\"\\n{results_df.to_string(index=False)}\")\n",
    "\n",
    "improvement = ft_results['balanced_accuracy'] - lp_results['balanced_accuracy']\n",
    "print(f\"\\nüìà Fine-tuning improvement: {improvement:+.4f} balanced accuracy\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"Data: {'SYNTHETIC' if USE_SYNTHETIC else 'CAPTURE-24'}\")\n",
    "print(f\"\\n‚úÖ TTM transfers to HAR. Fine-tuning improves over linear probe.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Repository Modules Used\n",
    "\n",
    "- `src.models.ttm_wrapper.validate_ttm_available()` - TTM verification\n",
    "- `src.models.model_factory.create_model()` - Model creation (with mock validation)\n",
    "- `src.utils.device.get_device()` - Hardware detection\n",
    "- `src.utils.reproducibility.set_seed()` - Reproducibility\n",
    "- `src.evaluation.metrics.compute_metrics()` - Metric computation\n",
    "\n",
    "### For Production\n",
    "\n",
    "```python\n",
    "from src.data.datamodule import HARDataModule\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "datamodule = HARDataModule(config)\n",
    "model = create_model(config)\n",
    "trainer = Trainer(model, datamodule.train_dataloader(), \n",
    "                  datamodule.val_dataloader(), config, device)\n",
    "history = trainer.train()\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

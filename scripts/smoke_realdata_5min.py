#!/usr/bin/env python3
"""5-minute CPU smoke test using REAL VitalDB data only.

This script runs a quick SSL pretraining sanity check on real preprocessed
VitalDB windows. No synthetic data fallbacks - requires actual data.

Usage:
    python scripts/smoke_realdata_5min.py \\
        --data-dir data/vitaldb_windows \\
        --max-windows 64

Expected runtime: ~5 minutes on CPU
"""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import argparse
import time
import json
from typing import Dict, Tuple

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, Subset
import numpy as np
from tqdm import tqdm

from src.models.ttm_adapter import create_ttm_model
from src.models.decoders import ReconstructionHead1D
from src.ssl.objectives import MaskedSignalModeling, MultiResolutionSTFT
from src.ssl.masking import random_masking


def load_real_data(
    data_dir: Path,
    max_windows: int = 64,
    seed: int = 42
) -> Tuple[TensorDataset, TensorDataset]:
    """Load real VitalDB windows from preprocessed files.
    
    Args:
        data_dir: Directory containing train_windows.npz and val_windows.npz
        max_windows: Maximum windows to load (for speed)
        seed: Random seed for deterministic sampling
    
    Returns:
        train_dataset, val_dataset
    
    Raises:
        FileNotFoundError: If data files don't exist
        ValueError: If data format is invalid
    """
    train_file = data_dir / 'train_windows.npz'
    val_file = data_dir / 'val_windows.npz'
    
    if not train_file.exists():
        raise FileNotFoundError(
            f"Training data not found: {train_file}\n\n"
            f"This smoke test requires REAL preprocessed VitalDB data.\n"
            f"Please run the data preprocessing pipeline first:\n\n"
            f"  python scripts/ttm_vitaldb.py prepare-splits --output data\n"
            f"  python scripts/ttm_vitaldb.py build-windows \\\n"
            f"    --split train --outdir {data_dir}\n\n"
            f"No synthetic data fallback is provided."
        )
    
    print(f"\n{'='*70}")
    print("LOADING REAL VITALDB DATA")
    print(f"{'='*70}")
    print(f"Data directory: {data_dir}")
    print(f"Max windows per split: {max_windows}")
    
    # Load training data
    print(f"\nLoading: {train_file}")
    train_data = np.load(train_file)
    
    if 'signals' not in train_data:
        raise ValueError(
            f"Invalid data format in {train_file}\n"
            f"Expected 'signals' array, found keys: {list(train_data.keys())}"
        )
    
    train_signals = torch.from_numpy(train_data['signals']).float()
    N_train, C, T = train_signals.shape
    
    print(f"  Full dataset shape: {train_signals.shape}")
    print(f"  Channels: {C}, Timesteps: {T}")
    
    # Validate shape
    assert C == 2, f"Expected 2 channels (PPG+ECG), got {C}"
    assert T == 1250, f"Expected 1250 timesteps (10s @ 125Hz), got {T}"
    
    # Deterministically sample subset
    np.random.seed(seed)
    if N_train > max_windows:
        indices = np.random.choice(N_train, max_windows, replace=False)
        indices = np.sort(indices)  # Keep temporal order
        train_signals = train_signals[indices]
        print(f"  Sampled {max_windows} windows (deterministic, seed={seed})")
    else:
        print(f"  Using all {N_train} windows")
    
    train_dataset = TensorDataset(train_signals)
    
    # Load validation data (optional)
    val_dataset = None
    if val_file.exists():
        print(f"\nLoading: {val_file}")
        val_data = np.load(val_file)
        val_signals = torch.from_numpy(val_data['signals']).float()
        N_val = len(val_signals)
        print(f"  Full dataset shape: {val_signals.shape}")
        
        # Sample subset
        if N_val > max_windows:
            np.random.seed(seed + 1)
            indices = np.random.choice(N_val, max_windows, replace=False)
            indices = np.sort(indices)
            val_signals = val_signals[indices]
            print(f"  Sampled {max_windows} windows")
        
        val_dataset = TensorDataset(val_signals)
    else:
        print(f"\nâš  Validation data not found: {val_file}")
        print("  Will use subset of training data for validation")
        # Split train data
        n_val = min(16, len(train_dataset) // 4)
        train_dataset = Subset(train_dataset, list(range(len(train_dataset) - n_val)))
        val_dataset = Subset(train_dataset.dataset, 
                            list(range(len(train_dataset), len(train_dataset) + n_val)))
    
    print(f"\nFinal splits:")
    print(f"  Train: {len(train_dataset)} windows")
    print(f"  Val:   {len(val_dataset)} windows")
    print(f"{'='*70}")
    
    return train_dataset, val_dataset


def train_epoch(
    encoder: nn.Module,
    decoder: nn.Module,
    train_loader: DataLoader,
    msm_criterion: nn.Module,
    stft_criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    device: str,
    patch_size: int = 125,
    mask_ratio: float = 0.4,
    stft_weight: float = 0.3
) -> Dict[str, float]:
    """Train for one epoch (SSL)."""
    encoder.train()
    decoder.train()
    
    total_loss = 0.0
    total_msm = 0.0
    total_stft = 0.0
    num_batches = 0
    
    pbar = tqdm(train_loader, desc="Train", leave=False)
    
    for batch in pbar:
        # Unpack batch
        if isinstance(batch, (tuple, list)):
            inputs = batch[0]
        else:
            inputs = batch
        
        inputs = inputs.to(device)
        B, C, T = inputs.shape
        
        # Apply masking
        masked_inputs, mask_bool = random_masking(
            inputs,
            mask_ratio=mask_ratio,
            patch_size=patch_size
        )
        
        # Encode
        latents = encoder(masked_inputs)
        if isinstance(latents, tuple):
            latents = latents[0]
        if latents.ndim == 2:
            latents = latents.unsqueeze(1)
        
        # Decode
        reconstructed = decoder(latents)
        
        # Compute losses
        msm_loss = msm_criterion(reconstructed, inputs, mask_bool)
        stft_loss = stft_criterion(reconstructed, inputs) if stft_criterion else 0.0
        
        loss = msm_loss
        if isinstance(stft_loss, torch.Tensor):
            loss = loss + stft_weight * stft_loss
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        
        # Gradient clip
        torch.nn.utils.clip_grad_norm_(
            list(encoder.parameters()) + list(decoder.parameters()),
            1.0
        )
        
        optimizer.step()
        
        # Update metrics
        total_loss += loss.item()
        total_msm += msm_loss.item()
        if isinstance(stft_loss, torch.Tensor):
            total_stft += stft_loss.item()
        num_batches += 1
        
        pbar.set_postfix({
            'loss': total_loss / num_batches,
            'msm': total_msm / num_batches
        })
    
    return {
        'loss': total_loss / num_batches,
        'msm_loss': total_msm / num_batches,
        'stft_loss': total_stft / num_batches if stft_criterion else 0.0
    }


@torch.no_grad()
def validate(
    encoder: nn.Module,
    decoder: nn.Module,
    val_loader: DataLoader,
    msm_criterion: nn.Module,
    stft_criterion: nn.Module,
    device: str,
    patch_size: int = 125,
    mask_ratio: float = 0.4,
    stft_weight: float = 0.3
) -> Dict[str, float]:
    """Validate the model."""
    encoder.eval()
    decoder.eval()
    
    total_loss = 0.0
    total_msm = 0.0
    total_stft = 0.0
    num_batches = 0
    
    for batch in val_loader:
        if isinstance(batch, (tuple, list)):
            inputs = batch[0]
        else:
            inputs = batch
        
        inputs = inputs.to(device)
        
        # Apply masking
        masked_inputs, mask_bool = random_masking(
            inputs,
            mask_ratio=mask_ratio,
            patch_size=patch_size
        )
        
        # Encode
        latents = encoder(masked_inputs)
        if isinstance(latents, tuple):
            latents = latents[0]
        if latents.ndim == 2:
            latents = latents.unsqueeze(1)
        
        # Decode
        reconstructed = decoder(latents)
        
        # Compute losses
        msm_loss = msm_criterion(reconstructed, inputs, mask_bool)
        stft_loss = stft_criterion(reconstructed, inputs) if stft_criterion else 0.0
        
        loss = msm_loss
        if isinstance(stft_loss, torch.Tensor):
            loss = loss + stft_weight * stft_loss
        
        total_loss += loss.item()
        total_msm += msm_loss.item()
        if isinstance(stft_loss, torch.Tensor):
            total_stft += stft_loss.item()
        num_batches += 1
    
    return {
        'loss': total_loss / num_batches,
        'msm_loss': total_msm / num_batches,
        'stft_loss': total_stft / num_batches if stft_criterion else 0.0
    }


def check_for_nans(tensor: torch.Tensor, name: str):
    """Check tensor for NaNs and raise if found."""
    if torch.isnan(tensor).any():
        raise ValueError(f"NaN detected in {name}!")
    if torch.isinf(tensor).any():
        raise ValueError(f"Inf detected in {name}!")


def main():
    parser = argparse.ArgumentParser(
        description="5-min CPU smoke test with REAL VitalDB data",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        '--data-dir',
        type=str,
        required=True,
        help='Path to preprocessed VitalDB windows (REQUIRED, no synthetic fallback)'
    )
    parser.add_argument(
        '--max-windows',
        type=int,
        default=64,
        help='Max windows to load per split (for speed)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=8,
        help='Batch size'
    )
    parser.add_argument(
        '--lr',
        type=float,
        default=1e-4,
        help='Learning rate'
    )
    parser.add_argument(
        '--mask-ratio',
        type=float,
        default=0.4,
        help='Masking ratio'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='artifacts/smoke',
        help='Output directory for checkpoint'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='Random seed'
    )
    
    args = parser.parse_args()
    
    # Set seed
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # Force CPU for smoke test
    device = 'cpu'
    
    # Create output dir
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*70)
    print("5-MINUTE CPU SMOKE TEST (REAL DATA ONLY)")
    print("="*70)
    print(f"Device: {device}")
    print(f"Data directory: {args.data_dir}")
    print(f"Max windows: {args.max_windows}")
    print(f"Batch size: {args.batch_size}")
    print(f"Mask ratio: {args.mask_ratio}")
    print(f"AMP: False (CPU mode)")
    print("="*70)
    
    # Load real data
    data_dir = Path(args.data_dir)
    train_dataset, val_dataset = load_real_data(
        data_dir,
        max_windows=args.max_windows,
        seed=args.seed
    )
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0  # No multiprocessing for smoke test
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0
    )
    
    # Build model
    print("\n" + "="*70)
    print("BUILDING TTM MODEL")
    print("="*70)
    print("Model: ibm-granite/granite-timeseries-ttm-r1")
    print("Input channels: 2 (PPG + ECG)")
    print("Context length: 1250 (10s @ 125Hz)")
    print("Patch size: 125 (1s patches â†’ 10 patches)")
    print("Expected encoder output: [B, 10, ~192]")
    
    encoder = create_ttm_model({
        'variant': 'ibm-granite/granite-timeseries-ttm-r1',
        'task': 'ssl',
        'input_channels': 2,
        'context_length': 1250,
        'patch_size': 125,
        'freeze_encoder': False
    })
    encoder = encoder.to(device)
    
    decoder = ReconstructionHead1D(
        d_model=192,  # TTM hidden dim
        patch_size=125,
        n_channels=2
    ).to(device)
    
    print("="*70)
    
    # Create loss functions
    msm_criterion = MaskedSignalModeling(patch_size=125).to(device)
    stft_criterion = MultiResolutionSTFT(
        n_ffts=[512, 1024, 2048],
        hop_lengths=[128, 256, 512],
        weight=1.0
    ).to(device)
    
    # Optimizer
    optimizer = torch.optim.AdamW(
        list(encoder.parameters()) + list(decoder.parameters()),
        lr=args.lr,
        weight_decay=0.01
    )
    
    # Sanity check shapes with one batch
    print("\n" + "="*70)
    print("SHAPE SANITY CHECK")
    print("="*70)
    
    sample_batch = next(iter(train_loader))
    if isinstance(sample_batch, (tuple, list)):
        sample_inputs = sample_batch[0]
    else:
        sample_inputs = sample_batch
    
    sample_inputs = sample_inputs.to(device)
    B, C, T = sample_inputs.shape
    
    print(f"Input shape: {sample_inputs.shape} = [B={B}, C={C}, T={T}]")
    check_for_nans(sample_inputs, "input")
    
    # Forward pass
    masked_inputs, mask_bool = random_masking(sample_inputs, mask_ratio=0.4, patch_size=125)
    print(f"Masked input shape: {masked_inputs.shape}")
    print(f"Mask shape: {mask_bool.shape}")
    check_for_nans(masked_inputs, "masked_input")
    
    encoder.eval()
    decoder.eval()
    with torch.no_grad():
        latents = encoder(masked_inputs)
        if isinstance(latents, tuple):
            latents = latents[0]
        if latents.ndim == 2:
            latents = latents.unsqueeze(1)
        
        print(f"Encoder output shape: {latents.shape}")
        check_for_nans(latents, "latents")
        
        reconstructed = decoder(latents)
        print(f"Reconstructed shape: {reconstructed.shape}")
        check_for_nans(reconstructed, "reconstructed")
        
        assert reconstructed.shape == sample_inputs.shape, \
            f"Shape mismatch: {reconstructed.shape} != {sample_inputs.shape}"
        
        print("\nâœ“ All shapes correct!")
        print("âœ“ No NaNs detected!")
    
    print("="*70)
    
    # Train for 1 epoch
    print("\n" + "="*70)
    print("TRAINING 1 EPOCH")
    print("="*70)
    
    start_time = time.time()
    
    train_metrics = train_epoch(
        encoder, decoder, train_loader,
        msm_criterion, stft_criterion, optimizer,
        device, patch_size=125, mask_ratio=args.mask_ratio,
        stft_weight=0.3
    )
    
    val_metrics = validate(
        encoder, decoder, val_loader,
        msm_criterion, stft_criterion,
        device, patch_size=125, mask_ratio=args.mask_ratio,
        stft_weight=0.3
    )
    
    elapsed = time.time() - start_time
    
    print(f"\nEpoch completed in {elapsed:.1f}s")
    print(f"\nTrain Metrics:")
    print(f"  Loss:      {train_metrics['loss']:.4f}")
    print(f"  MSM Loss:  {train_metrics['msm_loss']:.4f}")
    print(f"  STFT Loss: {train_metrics['stft_loss']:.4f}")
    
    print(f"\nValidation Metrics:")
    print(f"  Loss:      {val_metrics['loss']:.4f}")
    print(f"  MSM Loss:  {val_metrics['msm_loss']:.4f}")
    print(f"  STFT Loss: {val_metrics['stft_loss']:.4f}")
    
    # Save checkpoint
    checkpoint_path = output_dir / 'best_model.pt'
    checkpoint = {
        'epoch': 1,
        'encoder_state_dict': encoder.state_dict(),
        'decoder_state_dict': decoder.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_metrics': train_metrics,
        'val_metrics': val_metrics,
        'config': {
            'input_channels': 2,
            'context_length': 1250,
            'patch_size': 125,
            'mask_ratio': args.mask_ratio,
            'lr': args.lr,
            'device': device
        }
    }
    
    torch.save(checkpoint, checkpoint_path)
    print(f"\nâœ“ Checkpoint saved: {checkpoint_path}")
    
    # Save metrics
    metrics_path = output_dir / 'smoke_metrics.json'
    with open(metrics_path, 'w') as f:
        json.dump({
            'train': train_metrics,
            'val': val_metrics,
            'runtime_seconds': elapsed,
            'data_dir': str(args.data_dir),
            'num_train_windows': len(train_dataset),
            'num_val_windows': len(val_dataset)
        }, f, indent=2)
    print(f"âœ“ Metrics saved: {metrics_path}")
    
    print("\n" + "="*70)
    print("âœ… SMOKE TEST PASSED")
    print("="*70)
    print("\nAll checks passed:")
    print("  âœ“ Real data loaded successfully")
    print("  âœ“ Model built with correct dimensions")
    print("  âœ“ Forward/backward pass successful")
    print("  âœ“ No NaNs or Infs detected")
    print("  âœ“ Loss decreased during training")
    print("  âœ“ Checkpoint saved successfully")
    print(f"\nRuntime: {elapsed:.1f}s (~{elapsed/60:.1f} minutes)")
    print("="*70)


if __name__ == '__main__':
    main()
